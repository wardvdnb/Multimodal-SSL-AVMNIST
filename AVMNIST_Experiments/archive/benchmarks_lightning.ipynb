{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\anaconda3\\envs\\multibench\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from utils.reproducibility import set_seed\n",
    "set_seed(1)\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from models.unimodal import CentralUnimodalImage, CentralUnimodalAudio, UnimodalImage, UnimodalAudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils.get_data import get_dataloader_augmented, load_results_from_csv, AVMNISTDataModule, AVMNISTDinoDataModule, AVMNISTDataset\n",
    "from training_structures.unimodal import train as unimodal_train, test as unimodal_test\n",
    "from models.centralnet.centralnet import SimpleAV_CentralNet as CentralNet\n",
    "from training_structures.centralnet_train import train_centralnet, test_centralnet\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.visualisations import show_images, show_images_augmentations, \\\n",
    "evaluate_results, plot_training_results_from_csv, plot_training_results_from_csvs, \\\n",
    "pca_plot_multiclass, tsne_plot_multiclass, pca_plot_dataloaders\n",
    "from torchvision import transforms, datasets\n",
    "from torchmetrics.classification import Accuracy\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "import torch.multiprocessing\n",
    "# torch.multiprocessing.set_start_method('spawn')\n",
    "\n",
    "current_path = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_path)\n",
    "sys.path.append(current_path)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_dir = os.path.join(parent_dir, \"data/avmnist/\")\n",
    "dir_logs = os.path.join(parent_dir, \"supervised_results/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = AVMNISTDataModule(data_dir=data_dir, batch_size=128, num_workers=0)\n",
    "mnist_data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata,validdata,testdata = mnist_data.train_dataloader(), mnist_data.val_dataloader(), mnist_data.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_Unimodal:\n",
    "    def __init__(self):\n",
    "        self.criterion = nn.CrossEntropyLoss() # Loss function\n",
    "        self.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "        self.learning_rate = 0.001 # Initial learning rate\n",
    "        self.batch_size = 128       # Batch size\n",
    "        self.epochs = 1          # Total training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args_CentralNet:\n",
    "    def __init__(self):\n",
    "        self.channels = 16         # Base convolution channels\n",
    "        self.fusingmix = '11,32,53' # Fusion strategy\n",
    "        self.fusetype = 'wsum'     # Weighted sum fusion\n",
    "        self.num_outputs = 10      # Number of classes (AVMNIST)\n",
    "        self.criterion = nn.CrossEntropyLoss() # Loss function\n",
    "        self.use_cuda = torch.cuda.is_available()  # Use GPU if available\n",
    "        self.learning_rate = 0.001 # Initial learning rate\n",
    "        self.batch_size = 128       # Batch size\n",
    "        self.epochs = 1          # Total training epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unimodal Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [1, 2, 3]\n",
    "test_accuracies = []\n",
    "args_image = Args_Unimodal()\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f\"Running seed: {seed}\")\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Setup log dir for this run\n",
    "    run_dir = f\"{dir_logs}unimodal_image/seed_{seed}/\"\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    # Save last model only\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=run_dir,\n",
    "        filename='unimodal_image-last',\n",
    "        save_last=True\n",
    "    )\n",
    "\n",
    "    # Logger per seed\n",
    "    logger = CSVLogger(dir_logs, name=f\"unimodal_image/seed_{seed}\")\n",
    "\n",
    "    # Initialize model\n",
    "    image_model = UnimodalImage(with_head=True, num_epochs=args_image.epochs)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=args_image.epochs,\n",
    "        logger=logger,\n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator=\"gpu\",\n",
    "        deterministic=True,\n",
    "        # strategy=\"ddp\" if multi-GPU\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    trainer.fit(image_model, mnist_data)\n",
    "\n",
    "    trained_model = image_model.model\n",
    "\n",
    "    # Evaluate\n",
    "    modalnum = 0\n",
    "    test_log_file = os.path.join(run_dir, \"test_log.csv\")\n",
    "\n",
    "    avg_loss, accuracy, all_labels, all_probs = unimodal_test(\n",
    "        trained_model,\n",
    "        testdata,\n",
    "        args_image.criterion,\n",
    "        device,\n",
    "        modalnum=modalnum,\n",
    "        test_log_file=test_log_file\n",
    "    )\n",
    "\n",
    "    test_accuracies.append(accuracy)\n",
    "\n",
    "# Final summary\n",
    "mean_acc = np.mean(test_accuracies)\n",
    "std_acc = np.std(test_accuracies)\n",
    "\n",
    "print(f\"Mean Test Accuracy over {len(seeds)} seeds: {mean_acc:.4f}\")\n",
    "print(f\"Std Dev of Accuracy: {std_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_plot_path = f\"{dir_logs}unimodal_image/plots/\"\n",
    "\n",
    "class ImageWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Since centralnet returns image_out, audio_out, fusion_out, \n",
    "    we need to wrap it to only return the fusion_out. Since this is what we want to visualize.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, image, audio):\n",
    "        out = self.base_model(image) # Assuming the model takes audio first and then image\n",
    "        return out\n",
    "\n",
    "image_wrapper = ImageWrapper(trained_model)\n",
    "\n",
    "_ = pca_plot_dataloaders(image_wrapper, testdata, selected_digits=[5, 8], dirpath=pca_plot_path, \n",
    "                         show_plots=False, is_dino_based=False)\n",
    "_ = pca_plot_multiclass(image_wrapper, testdata, selected_digits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n",
    "                        dirpath=pca_plot_path, show_plots=False, is_dino_based=False)\n",
    "_ = tsne_plot_multiclass(image_wrapper, testdata, selected_digits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n",
    "                         dirpath=pca_plot_path, show_plots=False, random_seed=0, is_dino_based=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint( # save the last model\n",
    "    dirpath=f'{dir_logs}unimodal_image/',\n",
    "    filename='unimodal_image-last',\n",
    "    save_last=True,\n",
    ")\n",
    "logger = CSVLogger(dir_logs, name='unimodal_image')\n",
    "image_model = UnimodalImage(with_head=True, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddp = DDPStrategy()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"gpu\",\n",
    "    # strategy= ddp  # Add this line to enable data parallelism\n",
    "    )\n",
    "trainer.fit(image_model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_audio = Args_Unimodal()\n",
    "model = image_model.model\n",
    "modalnum = 0\n",
    "test_log_file = f'{dir_logs}unimodal_image/test_log.csv'\n",
    "_ = unimodal_test(model, testdata, args_audio.criterion, device, \n",
    "                    modalnum=modalnum, test_log_file=test_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unimodal Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint( # save the best model based on minimum validation loss?\n",
    "    monitor='val_loss',\n",
    "    dirpath='centralnet_results/unimodal_audio/',\n",
    "    filename='unimodal_audio-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='min'\n",
    ")\n",
    "logger = CSVLogger('centralnet_results/', name='unimodal_audio')\n",
    "audio_model = UnimodalAudio(with_head=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddp = DDPStrategy()\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=100,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    accelerator=\"gpu\",\n",
    "    # strategy= ddp  # Add this line to enable data parallelism\n",
    "    )\n",
    "trainer.fit(audio_model, mnist_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_audio = Args_Unimodal()\n",
    "model = audio_model.model\n",
    "_ = unimodal_test(model, testdata, args_audio.criterion, device, \n",
    "                    modalnum=modalnum, test_log_file=test_log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multimodal CentralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with augmentation type: burst_noise\n",
      "Epoch 1/1, Loss: 3.4985\n",
      "Validation Loss: 0.3932, Accuracy: 87.78%\n",
      "Saving Best\n",
      "Training Complete!\n",
      "Testing with augmentation type: burst_noise\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ward\\AppData\\Local\\Temp\\ipykernel_19340\\3076527283.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4429, Test Accuracy: 85.76%\n"
     ]
    }
   ],
   "source": [
    "args_central = Args_CentralNet()\n",
    "if __name__ == \"__main__\":\n",
    "    for aug_type in [\n",
    "        # \"aliased\", \n",
    "        \"burst_noise\", \n",
    "        # \"distorted\", \n",
    "        # \"extreme_noise\", \n",
    "        # \"multi_band\"\n",
    "        ]:\n",
    "        print(f\"Training with augmentation type: {aug_type}\")\n",
    "        model = CentralNet(args_central, audio_channels=1, image_channels=1).to(device)  # Assuming grayscale input for both\n",
    "        model_name = f'model_central_augmented_{aug_type}.pt'\n",
    "\n",
    "        dir_train_logs = \"training_logs/central/\"\n",
    "        if not os.path.exists(dir_train_logs):\n",
    "            os.makedirs(dir_train_logs)\n",
    "        log_file = f\"{dir_train_logs}training_log_central_{aug_type}.csv\"\n",
    "\n",
    "        model_name = train_centralnet(model, args_central, traindata, device, val_loader=validdata, \n",
    "                                      log_file=log_file, save_model=model_name)\n",
    "        \n",
    "        dir_test_logs = \"test_logs/central/\"\n",
    "        if not os.path.exists(dir_test_logs):\n",
    "            os.makedirs(dir_test_logs)\n",
    "        test_log_file = f\"{dir_test_logs}test_results_central_{aug_type}.csv\"\n",
    "\n",
    "        print(f\"Testing with augmentation type: {aug_type}\")\n",
    "        model = torch.load(model_name)\n",
    "        test_loss, test_accuracy, all_labels, all_probs = test_centralnet(model, testdata, \n",
    "                                                                        args_central.criterion, device, \n",
    "                                                                        test_log_file=test_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionOnlyWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Since centralnet returns image_out, audio_out, fusion_out, \n",
    "    we need to wrap it to only return the fusion_out. Since this is what we want to visualize.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, image, audio):\n",
    "        _, _, fusion_out = self.base_model(audio, image) # Assuming the model takes audio first and then image\n",
    "        return fusion_out\n",
    "\n",
    "central_wrapper = FusionOnlyWrapper(model)\n",
    "pca_plot_path = \"centralnet_results/plots/\"\n",
    "\n",
    "_ = pca_plot_dataloaders(central_wrapper, testdata, selected_digits=[5, 8], dirpath=pca_plot_path, \n",
    "                         show_plots=False, is_dino_based=False)\n",
    "_ = pca_plot_multiclass(central_wrapper, testdata, selected_digits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n",
    "                        dirpath=pca_plot_path, show_plots=False, is_dino_based=False)\n",
    "_ = tsne_plot_multiclass(central_wrapper, testdata, selected_digits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n",
    "                         dirpath=pca_plot_path, show_plots=False, random_seed=0, is_dino_based=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
