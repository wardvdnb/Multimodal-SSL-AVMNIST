{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import lightning.pytorch as pl\n",
    "import optuna\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping\n",
    "\n",
    "from update_config import update_hardware_config\n",
    "from objective_augment import objective\n",
    "from datetime import datetime\n",
    "import time\n",
    "from utils.debugging import add_debugging_to_lightning_module\n",
    "import torch\n",
    "import numpy as np\n",
    "import lightning.pytorch as pl\n",
    "from torchinfo import summary # For calculating GFLOPs\n",
    "\n",
    "from models.dino import MultiModalDINOLightning, CrossAttentionViTMultiModalEncoder, \\\n",
    "UniModalDINOLightning, ImageEncoder, \\\n",
    "SpectrogramEncoder, SpectrogramEncoderCentral, SpectrogramEncoderLSTM, SpectrogramEncoderResidual, \\\n",
    "SpectrogramEncoderViT, SpectrogramEncoderMobileViT, SpectrogramEncoderResNet\n",
    "from utils.get_data import AVMNISTDinoDataModule, TimeWarpWithStretch, GroupedMasking, GaussianNoise\n",
    "from datetime import datetime\n",
    "from utils.plots_trials import create_plots_for_study, load_all_versions, process_metrics, save_versions_to_csv, create_dir, plot_loss\n",
    "from utils.visualisations import pca_plot_dataloaders, pca_plot_multiclass, visualize_prediction_matrix\n",
    "from utils.get_data import get_dataloader_augmented\n",
    "from training_structures.dino_train import train_downstream, train_knn_classifier\n",
    "from optuna.storages import JournalStorage, JournalFileStorage, RetryFailedTrialCallback, RDBStorage\n",
    "import torchvision.transforms as transforms\n",
    "import torchaudio.transforms as audio_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalAugmentation:\n",
    "    def __init__(self, n_global_views=2, n_local_views=4, \n",
    "                 global_spec_size=112, local_spec_size=112, augment_values=None):\n",
    "        \n",
    "        self.n_local_views = n_local_views\n",
    "        self.n_global_views = n_global_views\n",
    "        self.global_spec_size = global_spec_size\n",
    "        self.local_spec_size = local_spec_size\n",
    "\n",
    "        self._initialize_transforms(augment_values)\n",
    "\n",
    "    def _initialize_transforms(self, augment_values=None):\n",
    "        global_image_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=28, scale=(0.75, 1.0), antialias=True),\n",
    "            transforms.RandomRotation(degrees=5),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        ])\n",
    "        local_image_transforms = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(size=28, scale=(0.3, 0.75), antialias=True),\n",
    "            transforms.RandomRotation(degrees=15),\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "            transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),\n",
    "        ])\n",
    "        if augment_values is None:\n",
    "            self.global_transforms = {\n",
    "                'image': global_image_transforms,\n",
    "                'audio': transforms.Compose([\n",
    "                    # Keep original size but apply moderate augmentations\n",
    "                    transforms.RandomResizedCrop(size=(self.global_spec_size, self.global_spec_size), scale=(0.8, 1.0), antialias=True),\n",
    "                    transforms.RandomApply([TimeWarpWithStretch(min_factor=0.9, max_factor=1.1, target_length=self.global_spec_size)], p=0.5),  # Add time-stretch (global)\n",
    "                    # Moderate frequency/time masking\n",
    "                    transforms.RandomApply([audio_transforms.FrequencyMasking(freq_mask_param=15)], p=0.3),\n",
    "                    transforms.RandomApply([audio_transforms.TimeMasking(time_mask_param=15)], p=0.3),\n",
    "                    # Add gentle pitch shifting (vertical shift in spectrogram)\n",
    "                    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "                    # Grouped Masking\n",
    "                    transforms.RandomApply([GroupedMasking(mask_ratio=0.25)], p=0.25),\n",
    "            ]),\n",
    "            }\n",
    "\n",
    "            self.local_transforms = {\n",
    "                'image': local_image_transforms,\n",
    "                'audio': transforms.Compose([\n",
    "                    # Apply crop to create local view but maintain overall dimensions\n",
    "                    transforms.RandomResizedCrop(size=(self.local_spec_size, self.local_spec_size), scale=(0.5, 0.9), antialias=True),\n",
    "                    transforms.RandomApply([TimeWarpWithStretch(min_factor=0.7, max_factor=1.3, target_length=self.local_spec_size)], p=0.7),  # Add time-stretch (local)\n",
    "                    # Much stronger frequency masking (vertical strips)\n",
    "                    transforms.RandomApply([audio_transforms.FrequencyMasking(freq_mask_param=40)], p=0.6),\n",
    "                    # Much stronger time masking (horizontal strips)\n",
    "                    transforms.RandomApply([audio_transforms.TimeMasking(time_mask_param=40)], p=0.6),\n",
    "                    # Multiple frequency masks\n",
    "                    # transforms.RandomApply([\n",
    "                    #     transforms.Compose([\n",
    "                    #         audio_transforms.FrequencyMasking(freq_mask_param=25),\n",
    "                    #         audio_transforms.FrequencyMasking(freq_mask_param=15)\n",
    "                    #     ])\n",
    "                    # ], p=0.25), # low p means it's a rare occurance that this transform is applied\n",
    "                    # Multiple time masks\n",
    "                    # transforms.RandomApply([\n",
    "                    #     transforms.Compose([\n",
    "                    #         audio_transforms.TimeMasking(time_mask_param=25),\n",
    "                    #         audio_transforms.TimeMasking(time_mask_param=15)\n",
    "                    #     ])\n",
    "                    # ], p=0.25),\n",
    "                    # Add more aggressive affine transformations (simulates pitch/time stretching in spectrogram)\n",
    "                    # transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.7, 1.3), shear=10),\n",
    "                    # Add noise\n",
    "                    transforms.RandomApply([GaussianNoise(std=0.2)], p=0.5),\n",
    "                    # Add random erasing (simulates dropout of frequency/time regions)\n",
    "                    # transforms.RandomErasing(p=0.5, scale=(0.02, 0.2)),\n",
    "                    # Add another random erasing for multiple masks\n",
    "                    # transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),\n",
    "                    # Grouped masking\n",
    "                    transforms.RandomApply([GroupedMasking(mask_ratio=0.75)], p=0.9),\n",
    "                ]),\n",
    "            }\n",
    "        else:\n",
    "            aug_to_class = {\n",
    "                \"time_warp\": TimeWarpWithStretch,\n",
    "                \"frequency_mask\": audio_transforms.FrequencyMasking,\n",
    "                \"time_mask\": audio_transforms.TimeMasking,\n",
    "                \"grouped_masking\": GroupedMasking,\n",
    "                \"gaussian_noise\": GaussianNoise,\n",
    "                # \"random_erasing\": transforms.RandomErasing,\n",
    "            }\n",
    "\n",
    "            augment_list = {\"global_views\": [], \"local_views\": []}\n",
    "            for aug_type in [\"global_views\", \"local_views\"]:\n",
    "                augmentations = augment_values['augmentations'][aug_type]\n",
    "                augmentation_probabilities = augment_values['augmentation_probabilities'][aug_type]\n",
    "                for aug in augmentations.keys():\n",
    "                    print(\"Making a transform:\")\n",
    "                    print(f\"Final transform: transforms.RandomApply({aug_to_class[aug]}({augmentations[aug]}), p={augmentation_probabilities[aug]})\")\n",
    "                    augment_list[aug_type].append(transforms.RandomApply([aug_to_class[aug](**augmentations[aug])], p=augmentation_probabilities[aug]))\n",
    "\n",
    "            self.global_transforms = {\n",
    "                'image': global_image_transforms,\n",
    "                'audio': transforms.Compose(augment_list['global_views']),\n",
    "            }\n",
    "\n",
    "            self.local_transforms = {\n",
    "                'image': local_image_transforms,\n",
    "                'audio': transforms.Compose(augment_list['local_views']),\n",
    "            }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, images, audios):\n",
    "        global_images = []\n",
    "        global_audios = []\n",
    "\n",
    "        for _ in range(self.n_global_views):\n",
    "            # Process global views\n",
    "            global_images.append(self.global_transforms['image'](images))\n",
    "            global_audios.append(self.global_transforms['audio'](audios))\n",
    "        \n",
    "        global_images = torch.stack(global_images, dim=0)\n",
    "        global_audios = torch.stack(global_audios, dim=0)\n",
    "\n",
    "        # Generate multiple diverse local views\n",
    "        local_images = []\n",
    "        local_audios = []\n",
    "        \n",
    "        for _ in range(self.n_local_views):\n",
    "            local_images.append(self.local_transforms['image'](images))\n",
    "            local_audios.append(self.local_transforms['audio'](audios))\n",
    "            \n",
    "        local_images = torch.stack(local_images, dim=0)\n",
    "        local_audios = torch.stack(local_audios, dim=0)\n",
    "        \n",
    "        return global_images, global_audios, local_images, local_audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated config: {'device': 'cpu', 'num_gpus': 0, 'num_workers': 4}\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'utils.get_data.TimeWarpWithStretch'>({'min_factor': 0.9, 'max_factor': 1.1}), p=0.3)\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'torchaudio.transforms._transforms.FrequencyMasking'>({'freq_mask_param': 5}), p=0.3)\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'torchaudio.transforms._transforms.TimeMasking'>({'time_mask_param': 5}), p=0.3)\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'utils.get_data.GaussianNoise'>({'std': 0.01}), p=0.1)\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'utils.get_data.TimeWarpWithStretch'>({'min_factor': 0.5, 'max_factor': 1.3}), p=0.8)\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'torchaudio.transforms._transforms.FrequencyMasking'>({'freq_mask_param': 20}), p=0.8)\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'torchaudio.transforms._transforms.TimeMasking'>({'time_mask_param': 20}), p=0.8)\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'utils.get_data.GaussianNoise'>({'std': 0.1}), p=0.8)\n",
      "Making a transform:\n",
      "Final transform: transforms.RandomApply(<class 'utils.get_data.GroupedMasking'>({'mask_ratio': 0.5}), p=0.8)\n",
      "{'augmentations': {'global_views': {'time_warp': {'min_factor': 0.9, 'max_factor': 1.1}, 'frequency_mask': {'freq_mask_param': 5}, 'time_mask': {'time_mask_param': 5}, 'gaussian_noise': {'std': 0.01}}, 'local_views': {'time_warp': {'min_factor': 0.5, 'max_factor': 1.3}, 'frequency_mask': {'freq_mask_param': 20}, 'time_mask': {'time_mask_param': 20}, 'gaussian_noise': {'std': 0.1}, 'grouped_masking': {'mask_ratio': 0.5}}}, 'augmentation_probabilities': {'global_views': {'time_warp': 0.3, 'frequency_mask': 0.3, 'time_mask': 0.3, 'gaussian_noise': 0.1}, 'local_views': {'time_warp': 0.8, 'frequency_mask': 0.8, 'time_mask': 0.8, 'gaussian_noise': 0.8, 'grouped_masking': 0.8}}}\n",
      "<__main__.MultiModalAugmentation object at 0x00000220E8018260>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\anaconda3\\envs\\multibench\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Ward\\anaconda3\\envs\\multibench\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(Path.cwd(), 'configs', 'config_augments.yaml')\n",
    "encoder_class = SpectrogramEncoderMobileViT\n",
    "\n",
    "config = yaml.safe_load(open(config_path))\n",
    "config = update_hardware_config(config)\n",
    "\n",
    "pl.seed_everything(config['experiment']['seed'], workers=True)\n",
    "\n",
    "# Model remains static throughout the search/experiment, however augments change dynamically based on config\n",
    "model = UniModalDINOLightning(\n",
    "        encoder_class = encoder_class,\n",
    "        data_dir=\"C:/Users/Ward/Desktop/vub-github/Thesis-project/AVMNIST_Experiments/data/avmnist/\", \n",
    "        dropout=config['hyperparameters']['dropout'],\n",
    "        learning_rate = config['hyperparameters']['learning_rate'],\n",
    "        projection_dim = config['hyperparameters']['projection_dim'],\n",
    "        output_dim = config['hyperparameters']['output_dim'],\n",
    "        momentum = config['hyperparameters']['momentum'],\n",
    "        center_momentum = config['hyperparameters']['center_momentum'],\n",
    "        teacher_temperature=config['hyperparameters']['teacher_temperature'],\n",
    "        weight_decay=config['hyperparameters']['weight_decay'],\n",
    "        cosine_loss_alpha=config['hyperparameters']['cosine_loss_alpha'],\n",
    "        num_epochs=config['hyperparameters']['num_epochs'],\n",
    "        data_augmentation=config['hyperparameters'].get('data_augmentation', 'burst_noise')\n",
    "    )\n",
    "\n",
    "def _process_augment_config(trial, config, is_hyperparameter_search=False):\n",
    "    \"\"\"\n",
    "    Process augmentation configuration for either:\n",
    "    - Hyperparameter search (using Optuna trial)\n",
    "    - Final training (using best_augments from config)\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial object (None for final training)\n",
    "        config: Full configuration dictionary\n",
    "        is_hyperparameter_search: Boolean flag for mode selection\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with:\n",
    "        - 'augmentations': Dict without probability parameters\n",
    "        - 'augmentation_probabilities': Dict with only probabilities\n",
    "    \"\"\"\n",
    "    if is_hyperparameter_search:\n",
    "        # Hyperparameter search mode - sample from ranges\n",
    "        augmentations = {'global_views': {}, 'local_views': {}}\n",
    "        augmentation_probabilities = {'global_views': {}, 'local_views': {}}\n",
    "\n",
    "        for view_setting in ['global_views', 'local_views']:\n",
    "            view_params = config[\"optuna\"][\"augmentations\"][view_setting].items()\n",
    "            for aug, params in view_params:\n",
    "                aug_params = {}\n",
    "                aug_prob = None\n",
    "                \n",
    "                for param_name, param_info in params.items():\n",
    "                    if param_name == \"p\":\n",
    "                        aug_prob = param_info[\"low\"]\n",
    "                        augmentation_probabilities[view_setting][aug] = aug_prob\n",
    "                    else:\n",
    "                        if param_info[\"type\"] == \"uniform\":\n",
    "                            aug_params[param_name] = param_info[\"low\"]\n",
    "                        elif param_info[\"type\"] == \"int\":\n",
    "                            aug_params[param_name] = param_info[\"low\"]\n",
    "                \n",
    "                if aug_params:  # Only add if there are non-p parameters\n",
    "                    augmentations[view_setting][aug] = aug_params\n",
    "\n",
    "        return {\n",
    "            \"augmentations\": augmentations,\n",
    "            \"augmentation_probabilities\": augmentation_probabilities\n",
    "        }\n",
    "    else:\n",
    "        # Final training mode - use fixed best_augments\n",
    "        if \"best_augments\" not in config:\n",
    "            raise ValueError(\"best_augments not found in config for final training\")\n",
    "            \n",
    "        # Create structures without 'p' parameters\n",
    "        augmentations = {\n",
    "            'global_views': {},\n",
    "            'local_views': {}\n",
    "        }\n",
    "        augmentation_probabilities = {\n",
    "            'global_views': {},\n",
    "            'local_views': {}\n",
    "        }\n",
    "        \n",
    "        for view_setting in ['global_views', 'local_views']:\n",
    "            for aug, params in config[\"best_augments\"][view_setting].items():\n",
    "                # Filter out 'p' from augmentations\n",
    "                aug_params = {k: v for k, v in params.items() if k != 'p'}\n",
    "                if aug_params:  # Only add if there are non-p parameters\n",
    "                    augmentations[view_setting][aug] = aug_params\n",
    "                \n",
    "                # Store probability separately\n",
    "                if 'p' in params:\n",
    "                    augmentation_probabilities[view_setting][aug] = params['p']\n",
    "        \n",
    "        return {\n",
    "            \"augmentations\": augmentations,\n",
    "            \"augmentation_probabilities\": augmentation_probabilities\n",
    "        }\n",
    "\n",
    "augment_values = _process_augment_config(None, config, is_hyperparameter_search=True)\n",
    "multimodal_augments = MultiModalAugmentation(augment_values=augment_values)\n",
    "print(augment_values)\n",
    "print(multimodal_augments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
