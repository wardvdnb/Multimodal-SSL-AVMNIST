{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder Code (Spectrograms only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../../\") # Add the parent directory to the path\n",
    "from utils.reproducibility import set_seed\n",
    "set_seed(1)\n",
    "import torch\n",
    "import os\n",
    "from utils.get_data import AVMNISTDataModule\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from models.dino import SpectrogramEncoder\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from training_structures.dino_train import feature_extraction_loop, compute_gflops, \\\n",
    "                                           train_knn_classifier, compute_classification_metrics, train_downstream\n",
    "from torchinfo import summary\n",
    "import copy\n",
    "import time\n",
    "from utils.visualisations import pca_plot_dataloaders, pca_plot_multiclass, tsne_plot_multiclass, visualize_prediction_matrix\n",
    "\n",
    "current_path = os.getcwd()\n",
    "root_path = \"../../\"\n",
    "data_dir=f'{root_path}/data/avmnist/'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definitions\n",
    "current_path = os.getcwd()\n",
    "# Set data loaders for evaluation\n",
    "avmnist_data = AVMNISTDataModule(\n",
    "    data_dir=data_dir, \n",
    "    num_workers=0, \n",
    "    batch_size=128,\n",
    ")\n",
    "avmnist_data.setup()\n",
    "train_loader,valid_loader,test_loader = \\\n",
    "    avmnist_data.train_dataloader(), avmnist_data.val_dataloader(), avmnist_data.test_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self, output_dim=256):\n",
    "        super().__init__()  # Use parent init\n",
    "\n",
    "        self.output_dim = output_dim  # Dimension of the latent space\n",
    "        \n",
    "        # **Encoder**\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # (1, 112, 112) → (32, 56, 56)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # (64, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # (128, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # (256, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # (256*7*7)\n",
    "            nn.Linear(256 * 7 * 7, output_dim)  # Project to latent_dim\n",
    "        )\n",
    "\n",
    "        # **Decoder**\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, 256 * 7 * 7),  # Expand from latent space\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 7, 7)),  # Reshape back to feature map\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),  \n",
    "            nn.Sigmoid()  # Output in range [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, spectrograms):\n",
    "        latent = self.encoder(spectrograms)\n",
    "        reconstructed = self.decoder(latent)\n",
    "        return reconstructed, latent  # Return both for further downstream use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_masking(spectrogram, mask_ratio=0.55, group_size=4):\n",
    "    \"\"\"\n",
    "    Randomly masks groups of patches in the input spectrogram.\n",
    "    \n",
    "    Args:\n",
    "        spectrogram: Input spectrogram of shape (batch_size, 1, height, width).\n",
    "        mask_ratio: Fraction of the spectrogram to mask.\n",
    "        group_size: Size of each group (e.g., 4x4 patches).\n",
    "    \n",
    "    Returns:\n",
    "        masked_spectrogram: Spectrogram with masked regions set to 0.\n",
    "        mask: Binary mask indicating the masked regions (1 = masked, 0 = unmasked).\n",
    "    \"\"\"\n",
    "    batch_size, _, height, width = spectrogram.shape\n",
    "    # print(f\"Input spectrogram shape: {spectrogram.shape}\")\n",
    "\n",
    "    # Ensure height and width are divisible by group_size\n",
    "    if height % group_size != 0 or width % group_size != 0:\n",
    "        raise ValueError(\"Height and width must be divisible by group_size\")\n",
    "\n",
    "    # Reshape spectrogram into groups\n",
    "    num_groups_h = height // group_size\n",
    "    num_groups_w = width // group_size\n",
    "    spectrogram = spectrogram.view(\n",
    "        batch_size, 1,\n",
    "        num_groups_h, group_size,\n",
    "        num_groups_w, group_size\n",
    "    )  # Shape: (batch_size, 1, num_groups_h, group_size, num_groups_w, group_size)\n",
    "    # print(f\"Spectrogram after grouping: {spectrogram.shape}\")\n",
    "\n",
    "    # Flatten the groups\n",
    "    spectrogram = spectrogram.permute(0, 1, 2, 4, 3, 5).contiguous()\n",
    "    spectrogram = spectrogram.view(batch_size, 1, num_groups_h * num_groups_w, group_size * group_size)\n",
    "    # print(f\"Spectrogram after flattening: {spectrogram.shape}\")\n",
    "\n",
    "    # Create a binary mask for groups\n",
    "    num_groups = num_groups_h * num_groups_w\n",
    "    num_masked_groups = int(mask_ratio * num_groups)\n",
    "    mask = torch.ones(batch_size, num_groups, device=spectrogram.device)\n",
    "    for i in range(batch_size):\n",
    "        masked_indices = torch.randperm(num_groups)[:num_masked_groups]\n",
    "        mask[i, masked_indices] = 0  # Set masked groups to 0\n",
    "\n",
    "    # Reshape the mask to match the grouped spectrogram\n",
    "    mask = mask.view(batch_size, num_groups_h, num_groups_w, 1)  # Shape: (batch_size, num_groups_h, num_groups_w, 1)\n",
    "    mask = mask.unsqueeze(-1)  # Shape: (batch_size, num_groups_h, num_groups_w, 1, 1)\n",
    "    mask = mask.unsqueeze(1)\n",
    "\n",
    "    # Reshape the spectrogram to match the mask\n",
    "    spectrogram = spectrogram.view(\n",
    "        batch_size, 1,\n",
    "        num_groups_h, num_groups_w,\n",
    "        group_size, group_size\n",
    "    )  # Shape: (batch_size, 1, num_groups_h, num_groups_w, group_size, group_size)\n",
    "\n",
    "    # Apply the mask to the spectrogram\n",
    "    masked_spectrogram = spectrogram * mask  # Broadcasting applies the mask correctly\n",
    "    # print(f\"Masked spectrogram before reshaping: {masked_spectrogram.shape}\")\n",
    "\n",
    "    # Reshape back to original spectrogram shape\n",
    "    masked_spectrogram = masked_spectrogram.permute(0, 1, 2, 4, 3, 5).contiguous()\n",
    "    masked_spectrogram = masked_spectrogram.view(batch_size, 1, height, width)\n",
    "    # print(f\"Final masked spectrogram shape: {masked_spectrogram.shape}\")\n",
    "\n",
    "    # Create the full binary mask for visualization or loss computation\n",
    "    mask = mask.view(batch_size, num_groups_h, num_groups_w, 1)\n",
    "    mask = mask.repeat(1, 1, 1, group_size * group_size)\n",
    "    mask = mask.view(batch_size, num_groups_h * group_size, num_groups_w * group_size)\n",
    "    mask = mask.view(batch_size, height, width)\n",
    "    # print(f\"Final mask shape: {mask.shape}\")\n",
    "\n",
    "    return masked_spectrogram, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of images | image: torch.Size([128, 1, 28, 28]) | audio: torch.Size([128, 1, 112, 112]) | label: torch.Size([128])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils.visualisations import show_images\n",
    "\n",
    "for image, audio, label in train_loader:\n",
    "    print(f\"Batch of images | image: {image.shape} | audio: {audio.shape} | label: {label.shape}\")\n",
    "    idx = 1\n",
    "    masked_spectrogram, mask = grouped_masking(audio[idx].unsqueeze(0), mask_ratio=0.75)\n",
    "    show_images(image[idx], masked_spectrogram.squeeze(0), label[idx])\n",
    "    break  # Test one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedConvAutoencoder(nn.Module):\n",
    "    def __init__(self, output_dim=256, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim  # Dimension of the latent space\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=2, padding=1),  # (1, 112, 112) → (32, 56, 56)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # (64, 28, 28)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # (128, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # (256, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # (256*7*7)\n",
    "            nn.Linear(256 * 7 * 7, output_dim)  # Project to latent dim\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, 256 * 7 * 7),  # Expand from latent space\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 7, 7)),  # Reshape back to feature map\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Output in range [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply random masking\n",
    "        masked_x, mask = grouped_masking(x, mask_ratio=self.mask_ratio)\n",
    "\n",
    "        # Encode the masked input\n",
    "        latent = self.encoder(masked_x)\n",
    "\n",
    "        # Decode to reconstruct the full spectrogram\n",
    "        reconstructed = self.decoder(latent)\n",
    "\n",
    "        return reconstructed, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.mobilenetv3 import mobilenet_v3_small\n",
    "\n",
    "class MobileVitMaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, output_dim=256, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim  # Dimension of the latent space\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.mobile_net = mobilenet_v3_small(weights=None)\n",
    "        # Modify first conv layer for single-channel input\n",
    "        self.mobile_net.features[0][0] = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        # Remove classifier head and replace with projection layer\n",
    "        self.mobile_net.classifier = nn.Identity()  # Remove MobileNet's original classifier\n",
    "        # Projection layer with dropout\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(576, 256),  # MobileNetV3 feature dim\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)  # Final embedding dim\n",
    "        )\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.mobile_net,\n",
    "            nn.Linear(576, 256),  # MobileNetV3 feature dim\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)  # Final embedding dim\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(output_dim, 256 * 7 * 7),  # Expand from latent space\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(1, (256, 7, 7)),  # Reshape back to feature map\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid()  # Output in range [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply random masking\n",
    "        masked_x, mask = grouped_masking(x, mask_ratio=self.mask_ratio)\n",
    "\n",
    "        # Encode the masked input\n",
    "        latent = self.encoder(masked_x)\n",
    "\n",
    "        # Decode to reconstruct the full spectrogram\n",
    "        reconstructed = self.decoder(latent)\n",
    "\n",
    "        return reconstructed, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper for the encoder part of the autoencoder to be used for feature extraction.\n",
    "    \"\"\"\n",
    "    def __init__(self, auto_encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = auto_encoder.encoder\n",
    "        self.output_dim = auto_encoder.output_dim  # Keep the output dimension\n",
    "\n",
    "    def forward(self, images=None, spectrograms=None):\n",
    "        if spectrograms is None:\n",
    "            raise ValueError(\"Input spectrograms must be provided for feature extraction.\")\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(spectrograms)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_autoencoder(model, dataloader, epochs=50, lr=1e-3, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for image, spectrograms, label in dataloader:\n",
    "            spectrograms = spectrograms.float().to(device)  # Expecting (batch_size, 1, 112, 112)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed, _ = model(spectrograms=spectrograms)\n",
    "\n",
    "            loss = criterion(reconstructed, spectrograms)  # Reconstruction loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_masked_autoencoder(model, dataloader, epochs=100, lr=1e-3, device=\"cuda\"):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()  # Reconstruction loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for image, spectrograms, label in dataloader:\n",
    "            spectrograms = spectrograms.float().to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            reconstructed, mask = model(spectrograms)\n",
    "\n",
    "            # Compute loss only for masked regions\n",
    "            loss = criterion(reconstructed * mask.unsqueeze(1), spectrograms * mask.unsqueeze(1))\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(auto_encoder, seeds=[1, 2, 3], model_name=\"autoencoder\", epochs=100, train_fn=None):\n",
    "\n",
    "    if train_fn is None:\n",
    "        raise ValueError(\"Please provide a training function.\")\n",
    "\n",
    "    initial_model_weights = copy.deepcopy(auto_encoder.state_dict())\n",
    "\n",
    "    gflops, params = compute_gflops(\n",
    "        model = auto_encoder, \n",
    "        input_data = [\n",
    "            torch.randn(1, 1, 112, 112),   # spectrograms (modality 2)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    test_accuracies_knn = []\n",
    "    test_accuracies_mlp = []\n",
    "    training_times = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        print(f\"Training with seed {seed}...\")\n",
    "        set_seed(seed)\n",
    "        auto_encoder.load_state_dict(copy.deepcopy(initial_model_weights))\n",
    "\n",
    "         # Start training timer\n",
    "        training_start = time.time()\n",
    "\n",
    "        train_fn(auto_encoder, train_loader, epochs=epochs, lr=1e-3, device=device)\n",
    "\n",
    "        # Calculate total training time\n",
    "        training_time = time.time() - training_start\n",
    "        \n",
    "        encoder = EncoderWrapper(auto_encoder)\n",
    "        _, knn_accuracy = train_knn_classifier(encoder, train_loader, test_loader, n_neighbors=5, device=device, is_dino_based=False)\n",
    "        mlp_classifier = train_downstream(\n",
    "            encoder,\n",
    "            train_loader,\n",
    "            valid_loader,\n",
    "            test_loader,\n",
    "            num_epochs=10,\n",
    "            device=device,\n",
    "            save_path=f'results/{model_name}/{model_name}_seed_{seed}.pt',\n",
    "            train_log_path=f'results/{model_name}/{model_name}_seed_{seed}_train_log.csv',\n",
    "            test_log_path=f'results/{model_name}/{model_name}_seed_{seed}_test_log.csv',\n",
    "            is_dino_based=False,\n",
    "        )\n",
    "        mlp_accuracy = compute_classification_metrics(mlp_classifier, test_loader, device)['accuracy']\n",
    "        test_accuracies_knn.append(knn_accuracy)\n",
    "        test_accuracies_mlp.append(mlp_accuracy)\n",
    "        training_times.append(training_time)\n",
    "\n",
    "    knn_acc = np.mean(test_accuracies_knn)\n",
    "    mlp_acc = np.mean(test_accuracies_mlp)\n",
    "    knn_acc_std = np.std(test_accuracies_knn)\n",
    "    mlp_acc_std = np.std(test_accuracies_mlp)\n",
    "    training_time = np.mean(training_times)\n",
    "    \n",
    "     # Also save a performance summary file\n",
    "    perf_summary = {\n",
    "        \"model_name\": model_name,\n",
    "        \"parameters\": f\"{params/1e6:.2f}M\",\n",
    "        \"gflops\": f\"{gflops:.2f}\",\n",
    "        \"seeds\": seeds,\n",
    "        \"training_time_hours\": f\"{training_time/3600:.2f}\",\n",
    "        \"downstream_mlp_acc\": f\"{mlp_acc:.4f}\",\n",
    "        \"downstream_knn_accuracy\": f\"{knn_acc:.4f}\",\n",
    "        \"downstream_mlp_acc_std\": f\"{mlp_acc_std:.4f}\",\n",
    "        \"downstream_knn_accuracy_std\": f\"{knn_acc_std:.4f}\",\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(\"results\", f\"{model_name}\", f\"{model_name}_performance_summary.txt\"), \"w\") as f:\n",
    "        for key, value in perf_summary.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "\n",
    "    _ = pca_plot_dataloaders(encoder, test_loader, selected_digits=[5, 8], dirpath=f\"results/{model_name}\", \n",
    "                            show_plots=False, is_dino_based=False)\n",
    "    _ = pca_plot_multiclass(encoder, test_loader, selected_digits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n",
    "                            dirpath=f\"results/{model_name}\", show_plots=False, is_dino_based=False)\n",
    "    _ = tsne_plot_multiclass(encoder, test_loader, selected_digits=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \n",
    "                            dirpath=f\"results/{model_name}\", show_plots=False, random_seed=0, is_dino_based=False)\n",
    "    _ = visualize_prediction_matrix(mlp_classifier, test_loader, dirpath=f\"results/{model_name}\", \n",
    "                                    show_plots=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = ConvAutoencoder(output_dim=512)\n",
    "training_loop(auto_encoder, seeds=[1, 2, 3], train_fn=train_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GFLOPs: 0.20 GFLOPs\n",
      "Model Parameters: 4.97 M parameters\n",
      "Training with seed 1...\n",
      "Epoch 1/100, Loss: 0.006971\n",
      "Epoch 2/100, Loss: 0.002768\n",
      "Epoch 3/100, Loss: 0.002324\n",
      "Epoch 4/100, Loss: 0.002148\n",
      "Epoch 5/100, Loss: 0.001986\n",
      "Epoch 6/100, Loss: 0.001891\n",
      "Epoch 7/100, Loss: 0.001819\n",
      "Epoch 8/100, Loss: 0.001732\n",
      "Epoch 9/100, Loss: 0.001685\n",
      "Epoch 10/100, Loss: 0.001632\n",
      "Epoch 11/100, Loss: 0.001568\n",
      "Epoch 12/100, Loss: 0.001513\n",
      "Epoch 13/100, Loss: 0.001486\n",
      "Epoch 14/100, Loss: 0.001448\n",
      "Epoch 15/100, Loss: 0.001405\n",
      "Epoch 16/100, Loss: 0.001373\n",
      "Epoch 17/100, Loss: 0.001355\n",
      "Epoch 18/100, Loss: 0.001325\n",
      "Epoch 19/100, Loss: 0.001297\n",
      "Epoch 20/100, Loss: 0.001266\n",
      "Epoch 21/100, Loss: 0.001254\n",
      "Epoch 22/100, Loss: 0.001232\n",
      "Epoch 23/100, Loss: 0.001214\n",
      "Epoch 24/100, Loss: 0.001192\n",
      "Epoch 25/100, Loss: 0.001174\n",
      "Epoch 26/100, Loss: 0.001148\n",
      "Epoch 27/100, Loss: 0.001135\n",
      "Epoch 28/100, Loss: 0.001111\n",
      "Epoch 29/100, Loss: 0.001098\n",
      "Epoch 30/100, Loss: 0.001087\n",
      "Epoch 31/100, Loss: 0.001065\n",
      "Epoch 32/100, Loss: 0.001052\n",
      "Epoch 33/100, Loss: 0.001042\n",
      "Epoch 34/100, Loss: 0.001032\n",
      "Epoch 35/100, Loss: 0.001015\n",
      "Epoch 36/100, Loss: 0.001000\n",
      "Epoch 37/100, Loss: 0.000995\n",
      "Epoch 38/100, Loss: 0.000989\n",
      "Epoch 39/100, Loss: 0.000975\n",
      "Epoch 40/100, Loss: 0.000965\n",
      "Epoch 41/100, Loss: 0.000950\n",
      "Epoch 42/100, Loss: 0.000949\n",
      "Epoch 43/100, Loss: 0.000938\n",
      "Epoch 44/100, Loss: 0.000926\n",
      "Epoch 45/100, Loss: 0.000921\n",
      "Epoch 46/100, Loss: 0.000913\n",
      "Epoch 47/100, Loss: 0.000902\n",
      "Epoch 48/100, Loss: 0.000889\n",
      "Epoch 49/100, Loss: 0.000880\n",
      "Epoch 50/100, Loss: 0.000879\n",
      "Epoch 51/100, Loss: 0.000864\n",
      "Epoch 52/100, Loss: 0.000860\n",
      "Epoch 53/100, Loss: 0.000857\n",
      "Epoch 54/100, Loss: 0.000850\n",
      "Epoch 55/100, Loss: 0.000844\n",
      "Epoch 56/100, Loss: 0.000832\n",
      "Epoch 57/100, Loss: 0.000828\n",
      "Epoch 58/100, Loss: 0.000828\n",
      "Epoch 59/100, Loss: 0.000821\n",
      "Epoch 60/100, Loss: 0.000819\n",
      "Epoch 61/100, Loss: 0.000805\n",
      "Epoch 62/100, Loss: 0.000808\n",
      "Epoch 63/100, Loss: 0.000800\n",
      "Epoch 64/100, Loss: 0.000791\n",
      "Epoch 65/100, Loss: 0.000785\n",
      "Epoch 66/100, Loss: 0.000786\n",
      "Epoch 67/100, Loss: 0.000776\n",
      "Epoch 68/100, Loss: 0.000772\n",
      "Epoch 69/100, Loss: 0.000769\n",
      "Epoch 70/100, Loss: 0.000769\n",
      "Epoch 71/100, Loss: 0.000757\n",
      "Epoch 72/100, Loss: 0.000759\n",
      "Epoch 73/100, Loss: 0.000748\n",
      "Epoch 74/100, Loss: 0.000747\n",
      "Epoch 75/100, Loss: 0.000743\n",
      "Epoch 76/100, Loss: 0.000741\n",
      "Epoch 77/100, Loss: 0.000736\n",
      "Epoch 78/100, Loss: 0.000731\n",
      "Epoch 79/100, Loss: 0.000727\n",
      "Epoch 80/100, Loss: 0.000724\n",
      "Epoch 81/100, Loss: 0.000723\n",
      "Epoch 82/100, Loss: 0.000720\n",
      "Epoch 83/100, Loss: 0.000711\n",
      "Epoch 84/100, Loss: 0.000714\n",
      "Epoch 85/100, Loss: 0.000705\n",
      "Epoch 86/100, Loss: 0.000707\n",
      "Epoch 87/100, Loss: 0.000705\n",
      "Epoch 88/100, Loss: 0.000703\n",
      "Epoch 89/100, Loss: 0.000699\n",
      "Epoch 90/100, Loss: 0.000700\n",
      "Epoch 91/100, Loss: 0.000697\n",
      "Epoch 92/100, Loss: 0.000693\n",
      "Epoch 93/100, Loss: 0.000690\n",
      "Epoch 94/100, Loss: 0.000690\n",
      "Epoch 95/100, Loss: 0.000689\n",
      "Epoch 96/100, Loss: 0.000685\n",
      "Epoch 97/100, Loss: 0.000685\n",
      "Epoch 98/100, Loss: 0.000684\n",
      "Epoch 99/100, Loss: 0.000679\n",
      "Epoch 100/100, Loss: 0.000676\n",
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:09<00:00, 43.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:04<00:00, 17.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\training_structures\\dino_train.py:229: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True) # For mixed precision training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy (k=5): 25.1800%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 430/430 [00:13<00:00, 33.07it/s, loss=1.81]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.8624, Val Loss: 1.7405, Val Acc: 38.00%\n",
      "Saved best model with validation accuracy: 38.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 430/430 [00:11<00:00, 35.98it/s, loss=1.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.7321, Val Loss: 1.7050, Val Acc: 39.54%\n",
      "Saved best model with validation accuracy: 39.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 430/430 [00:12<00:00, 35.58it/s, loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.6847, Val Loss: 1.6852, Val Acc: 39.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 430/430 [00:12<00:00, 35.20it/s, loss=1.59]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 1.6532, Val Loss: 1.6681, Val Acc: 40.24%\n",
      "Saved best model with validation accuracy: 40.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 430/430 [00:12<00:00, 35.19it/s, loss=1.68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 1.6271, Val Loss: 1.6432, Val Acc: 40.74%\n",
      "Saved best model with validation accuracy: 40.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 430/430 [00:12<00:00, 35.11it/s, loss=1.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 1.6013, Val Loss: 1.6312, Val Acc: 41.70%\n",
      "Saved best model with validation accuracy: 41.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 430/430 [00:12<00:00, 35.09it/s, loss=1.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 1.5809, Val Loss: 1.6212, Val Acc: 41.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 430/430 [00:11<00:00, 35.92it/s, loss=1.48]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 1.5644, Val Loss: 1.6137, Val Acc: 42.10%\n",
      "Saved best model with validation accuracy: 42.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 430/430 [00:11<00:00, 35.85it/s, loss=1.47]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 1.5511, Val Loss: 1.6125, Val Acc: 42.16%\n",
      "Saved best model with validation accuracy: 42.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 430/430 [00:12<00:00, 35.82it/s, loss=1.36]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 1.5427, Val Loss: 1.6123, Val Acc: 42.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\training_structures\\dino_train.py:317: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 37.52%\n",
      "Training with seed 2...\n",
      "Epoch 1/100, Loss: 0.006904\n",
      "Epoch 2/100, Loss: 0.002641\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m masked_autoencoder_mobilevit \u001b[38;5;241m=\u001b[39m MobileVitMaskedAutoencoder(output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, mask_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtraining_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_autoencoder_mobilevit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmasked_autoencoder_mobilevit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_masked_autoencoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 27\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(auto_encoder, seeds, model_name, epochs, train_fn)\u001b[0m\n\u001b[0;32m     24\u001b[0m  \u001b[38;5;66;03m# Start training timer\u001b[39;00m\n\u001b[0;32m     25\u001b[0m training_start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauto_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Calculate total training time\u001b[39;00m\n\u001b[0;32m     30\u001b[0m training_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m training_start\n",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m, in \u001b[0;36mtrain_masked_autoencoder\u001b[1;34m(model, dataloader, epochs, lr, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m spectrograms \u001b[38;5;241m=\u001b[39m spectrograms\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m reconstructed, mask \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspectrograms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Compute loss only for masked regions\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(reconstructed \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m), spectrograms \u001b[38;5;241m*\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Ward\\anaconda3\\envs\\multibench\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Ward\\anaconda3\\envs\\multibench\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[19], line 45\u001b[0m, in \u001b[0;36mMobileVitMaskedAutoencoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# Apply random masking\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     masked_x, mask \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped_masking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmask_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Encode the masked input\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     latent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(masked_x)\n",
      "Cell \u001b[1;32mIn[16], line 42\u001b[0m, in \u001b[0;36mgrouped_masking\u001b[1;34m(spectrogram, mask_ratio, group_size)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m     41\u001b[0m     masked_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandperm(num_groups)[:num_masked_groups]\n\u001b[1;32m---> 42\u001b[0m     mask[i, masked_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Set masked groups to 0\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Reshape the mask to match the grouped spectrogram\u001b[39;00m\n\u001b[0;32m     45\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mview(batch_size, num_groups_h, num_groups_w, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape: (batch_size, num_groups_h, num_groups_w, 1)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "masked_autoencoder_mobilevit = MobileVitMaskedAutoencoder(output_dim=256, mask_ratio=0.75)\n",
    "training_loop(masked_autoencoder_mobilevit, seeds=[1, 2, 3], model_name=\"masked_autoencoder_mobilevit\", epochs=100, train_fn=train_masked_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model GFLOPs: 0.23 GFLOPs\n",
      "Model Parameters: 7.21 M parameters\n",
      "Training with seed 1...\n",
      "Epoch 1/100, Loss: 0.005044\n",
      "Epoch 2/100, Loss: 0.001451\n",
      "Epoch 3/100, Loss: 0.001207\n",
      "Epoch 4/100, Loss: 0.001047\n",
      "Epoch 5/100, Loss: 0.000934\n",
      "Epoch 6/100, Loss: 0.000870\n",
      "Epoch 7/100, Loss: 0.000822\n",
      "Epoch 8/100, Loss: 0.000774\n",
      "Epoch 9/100, Loss: 0.000737\n",
      "Epoch 10/100, Loss: 0.000711\n",
      "Epoch 11/100, Loss: 0.000687\n",
      "Epoch 12/100, Loss: 0.000668\n",
      "Epoch 13/100, Loss: 0.000655\n",
      "Epoch 14/100, Loss: 0.000639\n",
      "Epoch 15/100, Loss: 0.000625\n",
      "Epoch 16/100, Loss: 0.000613\n",
      "Epoch 17/100, Loss: 0.000603\n",
      "Epoch 18/100, Loss: 0.000593\n",
      "Epoch 19/100, Loss: 0.000584\n",
      "Epoch 20/100, Loss: 0.000576\n",
      "Epoch 21/100, Loss: 0.000568\n",
      "Epoch 22/100, Loss: 0.000560\n",
      "Epoch 23/100, Loss: 0.000556\n",
      "Epoch 24/100, Loss: 0.000550\n",
      "Epoch 25/100, Loss: 0.000545\n",
      "Epoch 26/100, Loss: 0.000539\n",
      "Epoch 27/100, Loss: 0.000534\n",
      "Epoch 28/100, Loss: 0.000528\n",
      "Epoch 29/100, Loss: 0.000523\n",
      "Epoch 30/100, Loss: 0.000521\n",
      "Epoch 31/100, Loss: 0.000518\n",
      "Epoch 32/100, Loss: 0.000510\n",
      "Epoch 33/100, Loss: 0.000506\n",
      "Epoch 34/100, Loss: 0.000503\n",
      "Epoch 35/100, Loss: 0.000501\n",
      "Epoch 36/100, Loss: 0.000497\n",
      "Epoch 37/100, Loss: 0.000496\n",
      "Epoch 38/100, Loss: 0.000490\n",
      "Epoch 39/100, Loss: 0.000487\n",
      "Epoch 40/100, Loss: 0.000485\n",
      "Epoch 41/100, Loss: 0.000482\n",
      "Epoch 42/100, Loss: 0.000479\n",
      "Epoch 43/100, Loss: 0.000537\n",
      "Epoch 44/100, Loss: 0.000478\n",
      "Epoch 45/100, Loss: 0.000475\n",
      "Epoch 46/100, Loss: 0.000470\n",
      "Epoch 47/100, Loss: 0.000470\n",
      "Epoch 48/100, Loss: 0.000466\n",
      "Epoch 49/100, Loss: 0.000464\n",
      "Epoch 50/100, Loss: 0.000463\n",
      "Epoch 51/100, Loss: 0.000460\n",
      "Epoch 52/100, Loss: 0.000458\n",
      "Epoch 53/100, Loss: 0.000457\n",
      "Epoch 54/100, Loss: 0.000455\n",
      "Epoch 55/100, Loss: 0.000453\n",
      "Epoch 56/100, Loss: 0.000451\n",
      "Epoch 57/100, Loss: 0.000451\n",
      "Epoch 58/100, Loss: 0.000448\n",
      "Epoch 59/100, Loss: 0.000448\n",
      "Epoch 60/100, Loss: 0.000445\n",
      "Epoch 61/100, Loss: 0.000444\n",
      "Epoch 62/100, Loss: 0.000442\n",
      "Epoch 63/100, Loss: 0.000440\n",
      "Epoch 64/100, Loss: 0.000440\n",
      "Epoch 65/100, Loss: 0.000437\n",
      "Epoch 66/100, Loss: 0.000436\n",
      "Epoch 67/100, Loss: 0.000435\n",
      "Epoch 68/100, Loss: 0.000433\n",
      "Epoch 69/100, Loss: 0.000432\n",
      "Epoch 70/100, Loss: 0.000432\n",
      "Epoch 71/100, Loss: 0.000430\n",
      "Epoch 72/100, Loss: 0.000429\n",
      "Epoch 73/100, Loss: 0.000427\n",
      "Epoch 74/100, Loss: 0.000426\n",
      "Epoch 75/100, Loss: 0.000425\n",
      "Epoch 76/100, Loss: 0.000424\n",
      "Epoch 77/100, Loss: 0.000423\n",
      "Epoch 78/100, Loss: 0.000422\n",
      "Epoch 79/100, Loss: 0.000421\n",
      "Epoch 80/100, Loss: 0.000420\n",
      "Epoch 81/100, Loss: 0.000419\n",
      "Epoch 82/100, Loss: 0.000419\n",
      "Epoch 83/100, Loss: 0.000417\n",
      "Epoch 84/100, Loss: 0.000416\n",
      "Epoch 85/100, Loss: 0.000415\n",
      "Epoch 86/100, Loss: 0.000414\n",
      "Epoch 87/100, Loss: 0.000415\n",
      "Epoch 88/100, Loss: 0.000412\n",
      "Epoch 89/100, Loss: 0.000411\n",
      "Epoch 90/100, Loss: 0.000411\n",
      "Epoch 91/100, Loss: 0.000410\n",
      "Epoch 92/100, Loss: 0.000409\n",
      "Epoch 93/100, Loss: 0.000409\n",
      "Epoch 94/100, Loss: 0.000409\n",
      "Epoch 95/100, Loss: 0.000407\n",
      "Epoch 96/100, Loss: 0.000406\n",
      "Epoch 97/100, Loss: 0.000406\n",
      "Epoch 98/100, Loss: 0.000405\n",
      "Epoch 99/100, Loss: 0.000404\n",
      "Epoch 100/100, Loss: 0.000404\n",
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:09<00:00, 46.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 42.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\training_structures\\dino_train.py:197: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True) # For mixed precision training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy (k=5): 30.3200%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 430/430 [00:09<00:00, 44.66it/s, loss=0.874]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.9946, Val Loss: 0.7267, Val Acc: 75.70%\n",
      "Saved best model with validation accuracy: 75.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 430/430 [00:10<00:00, 42.75it/s, loss=0.829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.6784, Val Loss: 0.6268, Val Acc: 78.66%\n",
      "Saved best model with validation accuracy: 78.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 430/430 [00:10<00:00, 42.67it/s, loss=0.59] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.5824, Val Loss: 0.5736, Val Acc: 80.92%\n",
      "Saved best model with validation accuracy: 80.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 430/430 [00:09<00:00, 43.07it/s, loss=0.565]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.5128, Val Loss: 0.5234, Val Acc: 82.92%\n",
      "Saved best model with validation accuracy: 82.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 430/430 [00:09<00:00, 43.06it/s, loss=0.494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.4589, Val Loss: 0.4951, Val Acc: 83.02%\n",
      "Saved best model with validation accuracy: 83.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 430/430 [00:09<00:00, 43.13it/s, loss=0.554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.4169, Val Loss: 0.4795, Val Acc: 83.82%\n",
      "Saved best model with validation accuracy: 83.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 430/430 [00:10<00:00, 41.64it/s, loss=0.47] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.3852, Val Loss: 0.4643, Val Acc: 83.90%\n",
      "Saved best model with validation accuracy: 83.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 430/430 [00:10<00:00, 42.31it/s, loss=0.338]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.3618, Val Loss: 0.4574, Val Acc: 84.52%\n",
      "Saved best model with validation accuracy: 84.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 430/430 [00:10<00:00, 42.46it/s, loss=0.397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.3466, Val Loss: 0.4524, Val Acc: 84.90%\n",
      "Saved best model with validation accuracy: 84.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 430/430 [00:10<00:00, 41.77it/s, loss=0.433]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.3376, Val Loss: 0.4508, Val Acc: 84.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\training_structures\\dino_train.py:285: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 80.61%\n",
      "Training with seed 2...\n",
      "Epoch 1/100, Loss: 0.005104\n",
      "Epoch 2/100, Loss: 0.001461\n",
      "Epoch 3/100, Loss: 0.001202\n",
      "Epoch 4/100, Loss: 0.001027\n",
      "Epoch 5/100, Loss: 0.000926\n",
      "Epoch 6/100, Loss: 0.000854\n",
      "Epoch 7/100, Loss: 0.000800\n",
      "Epoch 8/100, Loss: 0.000757\n",
      "Epoch 9/100, Loss: 0.000723\n",
      "Epoch 10/100, Loss: 0.000699\n",
      "Epoch 11/100, Loss: 0.000677\n",
      "Epoch 12/100, Loss: 0.000660\n",
      "Epoch 13/100, Loss: 0.000648\n",
      "Epoch 14/100, Loss: 0.000631\n",
      "Epoch 15/100, Loss: 0.000621\n",
      "Epoch 16/100, Loss: 0.000607\n",
      "Epoch 17/100, Loss: 0.000594\n",
      "Epoch 18/100, Loss: 0.000587\n",
      "Epoch 19/100, Loss: 0.000580\n",
      "Epoch 20/100, Loss: 0.000570\n",
      "Epoch 21/100, Loss: 0.000560\n",
      "Epoch 22/100, Loss: 0.000556\n",
      "Epoch 23/100, Loss: 0.000549\n",
      "Epoch 24/100, Loss: 0.000543\n",
      "Epoch 25/100, Loss: 0.000538\n",
      "Epoch 26/100, Loss: 0.000535\n",
      "Epoch 27/100, Loss: 0.000527\n",
      "Epoch 28/100, Loss: 0.000522\n",
      "Epoch 29/100, Loss: 0.000517\n",
      "Epoch 30/100, Loss: 0.000515\n",
      "Epoch 31/100, Loss: 0.000509\n",
      "Epoch 32/100, Loss: 0.000505\n",
      "Epoch 33/100, Loss: 0.000501\n",
      "Epoch 34/100, Loss: 0.000499\n",
      "Epoch 35/100, Loss: 0.000494\n",
      "Epoch 36/100, Loss: 0.000491\n",
      "Epoch 37/100, Loss: 0.000488\n",
      "Epoch 38/100, Loss: 0.000486\n",
      "Epoch 39/100, Loss: 0.000481\n",
      "Epoch 40/100, Loss: 0.000478\n",
      "Epoch 41/100, Loss: 0.000476\n",
      "Epoch 42/100, Loss: 0.000473\n",
      "Epoch 43/100, Loss: 0.000470\n",
      "Epoch 44/100, Loss: 0.000468\n",
      "Epoch 45/100, Loss: 0.000466\n",
      "Epoch 46/100, Loss: 0.000463\n",
      "Epoch 47/100, Loss: 0.000460\n",
      "Epoch 48/100, Loss: 0.000461\n",
      "Epoch 49/100, Loss: 0.000457\n",
      "Epoch 50/100, Loss: 0.000455\n",
      "Epoch 51/100, Loss: 0.000453\n",
      "Epoch 52/100, Loss: 0.000451\n",
      "Epoch 53/100, Loss: 0.000449\n",
      "Epoch 54/100, Loss: 0.000448\n",
      "Epoch 55/100, Loss: 0.000446\n",
      "Epoch 56/100, Loss: 0.000445\n",
      "Epoch 57/100, Loss: 0.000442\n",
      "Epoch 58/100, Loss: 0.000442\n",
      "Epoch 59/100, Loss: 0.000440\n",
      "Epoch 60/100, Loss: 0.000437\n",
      "Epoch 61/100, Loss: 0.000437\n",
      "Epoch 62/100, Loss: 0.000435\n",
      "Epoch 63/100, Loss: 0.000434\n",
      "Epoch 64/100, Loss: 0.000432\n",
      "Epoch 65/100, Loss: 0.000431\n",
      "Epoch 66/100, Loss: 0.000429\n",
      "Epoch 67/100, Loss: 0.000428\n",
      "Epoch 68/100, Loss: 0.000427\n",
      "Epoch 69/100, Loss: 0.000426\n",
      "Epoch 70/100, Loss: 0.000424\n",
      "Epoch 71/100, Loss: 0.000424\n",
      "Epoch 72/100, Loss: 0.000422\n",
      "Epoch 73/100, Loss: 0.000422\n",
      "Epoch 74/100, Loss: 0.000420\n",
      "Epoch 75/100, Loss: 0.000420\n",
      "Epoch 76/100, Loss: 0.000418\n",
      "Epoch 77/100, Loss: 0.000416\n",
      "Epoch 78/100, Loss: 0.000416\n",
      "Epoch 79/100, Loss: 0.000416\n",
      "Epoch 80/100, Loss: 0.000415\n",
      "Epoch 81/100, Loss: 0.000413\n",
      "Epoch 82/100, Loss: 0.000413\n",
      "Epoch 83/100, Loss: 0.000413\n",
      "Epoch 84/100, Loss: 0.000411\n",
      "Epoch 85/100, Loss: 0.000410\n",
      "Epoch 86/100, Loss: 0.000409\n",
      "Epoch 87/100, Loss: 0.000409\n",
      "Epoch 88/100, Loss: 0.000407\n",
      "Epoch 89/100, Loss: 0.000407\n",
      "Epoch 90/100, Loss: 0.000406\n",
      "Epoch 91/100, Loss: 0.000406\n",
      "Epoch 92/100, Loss: 0.000405\n",
      "Epoch 93/100, Loss: 0.000404\n",
      "Epoch 94/100, Loss: 0.000404\n",
      "Epoch 95/100, Loss: 0.000404\n",
      "Epoch 96/100, Loss: 0.000403\n",
      "Epoch 97/100, Loss: 0.000402\n",
      "Epoch 98/100, Loss: 0.000401\n",
      "Epoch 99/100, Loss: 0.000401\n",
      "Epoch 100/100, Loss: 0.000400\n",
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:10<00:00, 42.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 42.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\training_structures\\dino_train.py:197: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True) # For mixed precision training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy (k=5): 28.4900%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 430/430 [00:09<00:00, 43.18it/s, loss=0.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.0104, Val Loss: 0.7238, Val Acc: 75.10%\n",
      "Saved best model with validation accuracy: 75.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 430/430 [00:10<00:00, 40.28it/s, loss=0.709]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.6803, Val Loss: 0.6222, Val Acc: 79.42%\n",
      "Saved best model with validation accuracy: 79.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 430/430 [00:10<00:00, 40.50it/s, loss=0.649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.5848, Val Loss: 0.5670, Val Acc: 81.12%\n",
      "Saved best model with validation accuracy: 81.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 430/430 [00:10<00:00, 39.79it/s, loss=0.508]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.5125, Val Loss: 0.5222, Val Acc: 82.24%\n",
      "Saved best model with validation accuracy: 82.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 430/430 [00:10<00:00, 42.24it/s, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.4573, Val Loss: 0.4903, Val Acc: 83.16%\n",
      "Saved best model with validation accuracy: 83.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 430/430 [00:10<00:00, 41.35it/s, loss=0.335]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.4150, Val Loss: 0.4751, Val Acc: 83.90%\n",
      "Saved best model with validation accuracy: 83.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 430/430 [00:10<00:00, 40.94it/s, loss=0.438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.3830, Val Loss: 0.4589, Val Acc: 84.26%\n",
      "Saved best model with validation accuracy: 84.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 430/430 [00:11<00:00, 38.96it/s, loss=0.373]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.3590, Val Loss: 0.4540, Val Acc: 84.78%\n",
      "Saved best model with validation accuracy: 84.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 430/430 [00:09<00:00, 46.26it/s, loss=0.306]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.3433, Val Loss: 0.4477, Val Acc: 84.88%\n",
      "Saved best model with validation accuracy: 84.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 430/430 [00:09<00:00, 43.86it/s, loss=0.291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.3343, Val Loss: 0.4468, Val Acc: 85.08%\n",
      "Saved best model with validation accuracy: 85.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\training_structures\\dino_train.py:285: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 81.03%\n",
      "Training with seed 3...\n",
      "Epoch 1/100, Loss: 0.004788\n",
      "Epoch 2/100, Loss: 0.001441\n",
      "Epoch 3/100, Loss: 0.001177\n",
      "Epoch 4/100, Loss: 0.001020\n",
      "Epoch 5/100, Loss: 0.000925\n",
      "Epoch 6/100, Loss: 0.000862\n",
      "Epoch 7/100, Loss: 0.000811\n",
      "Epoch 8/100, Loss: 0.000765\n",
      "Epoch 9/100, Loss: 0.000732\n",
      "Epoch 10/100, Loss: 0.000706\n",
      "Epoch 11/100, Loss: 0.000680\n",
      "Epoch 12/100, Loss: 0.000662\n",
      "Epoch 13/100, Loss: 0.000644\n",
      "Epoch 14/100, Loss: 0.000629\n",
      "Epoch 15/100, Loss: 0.000613\n",
      "Epoch 16/100, Loss: 0.000602\n",
      "Epoch 17/100, Loss: 0.000594\n",
      "Epoch 18/100, Loss: 0.000582\n",
      "Epoch 19/100, Loss: 0.000577\n",
      "Epoch 20/100, Loss: 0.000566\n",
      "Epoch 21/100, Loss: 0.000562\n",
      "Epoch 22/100, Loss: 0.000554\n",
      "Epoch 23/100, Loss: 0.000548\n",
      "Epoch 24/100, Loss: 0.000542\n",
      "Epoch 25/100, Loss: 0.000537\n",
      "Epoch 26/100, Loss: 0.000532\n",
      "Epoch 27/100, Loss: 0.000528\n",
      "Epoch 28/100, Loss: 0.000523\n",
      "Epoch 29/100, Loss: 0.000518\n",
      "Epoch 30/100, Loss: 0.000513\n",
      "Epoch 31/100, Loss: 0.000509\n",
      "Epoch 32/100, Loss: 0.000506\n",
      "Epoch 33/100, Loss: 0.000502\n",
      "Epoch 34/100, Loss: 0.000498\n",
      "Epoch 35/100, Loss: 0.000496\n",
      "Epoch 36/100, Loss: 0.000491\n",
      "Epoch 37/100, Loss: 0.000488\n",
      "Epoch 38/100, Loss: 0.000486\n",
      "Epoch 39/100, Loss: 0.000483\n",
      "Epoch 40/100, Loss: 0.000481\n",
      "Epoch 41/100, Loss: 0.000477\n",
      "Epoch 42/100, Loss: 0.000475\n",
      "Epoch 43/100, Loss: 0.000471\n",
      "Epoch 44/100, Loss: 0.000471\n",
      "Epoch 45/100, Loss: 0.000468\n",
      "Epoch 46/100, Loss: 0.000466\n",
      "Epoch 47/100, Loss: 0.000463\n",
      "Epoch 48/100, Loss: 0.000461\n",
      "Epoch 49/100, Loss: 0.000459\n",
      "Epoch 50/100, Loss: 0.000457\n",
      "Epoch 51/100, Loss: 0.000456\n",
      "Epoch 52/100, Loss: 0.000453\n",
      "Epoch 53/100, Loss: 0.000452\n",
      "Epoch 54/100, Loss: 0.000450\n",
      "Epoch 55/100, Loss: 0.000449\n",
      "Epoch 56/100, Loss: 0.000446\n",
      "Epoch 57/100, Loss: 0.000445\n",
      "Epoch 58/100, Loss: 0.000443\n",
      "Epoch 59/100, Loss: 0.000442\n",
      "Epoch 60/100, Loss: 0.000440\n",
      "Epoch 61/100, Loss: 0.000440\n",
      "Epoch 62/100, Loss: 0.000437\n",
      "Epoch 63/100, Loss: 0.000436\n",
      "Epoch 64/100, Loss: 0.000435\n",
      "Epoch 65/100, Loss: 0.000433\n",
      "Epoch 66/100, Loss: 0.000432\n",
      "Epoch 67/100, Loss: 0.000431\n",
      "Epoch 68/100, Loss: 0.000431\n",
      "Epoch 69/100, Loss: 0.000429\n",
      "Epoch 70/100, Loss: 0.000428\n",
      "Epoch 71/100, Loss: 0.000426\n",
      "Epoch 72/100, Loss: 0.000426\n",
      "Epoch 73/100, Loss: 0.000423\n",
      "Epoch 74/100, Loss: 0.000423\n",
      "Epoch 75/100, Loss: 0.000422\n",
      "Epoch 76/100, Loss: 0.000421\n",
      "Epoch 77/100, Loss: 0.000419\n",
      "Epoch 78/100, Loss: 0.000419\n",
      "Epoch 79/100, Loss: 0.000418\n",
      "Epoch 80/100, Loss: 0.000416\n",
      "Epoch 81/100, Loss: 0.000416\n",
      "Epoch 82/100, Loss: 0.000415\n",
      "Epoch 83/100, Loss: 0.000415\n",
      "Epoch 84/100, Loss: 0.000412\n",
      "Epoch 85/100, Loss: 0.000413\n",
      "Epoch 86/100, Loss: 0.000411\n",
      "Epoch 87/100, Loss: 0.000410\n",
      "Epoch 88/100, Loss: 0.000410\n",
      "Epoch 89/100, Loss: 0.000410\n",
      "Epoch 90/100, Loss: 0.000409\n",
      "Epoch 91/100, Loss: 0.000408\n",
      "Epoch 92/100, Loss: 0.000406\n",
      "Epoch 93/100, Loss: 0.000405\n",
      "Epoch 94/100, Loss: 0.000406\n",
      "Epoch 95/100, Loss: 0.000405\n",
      "Epoch 96/100, Loss: 0.000405\n",
      "Epoch 97/100, Loss: 0.000404\n",
      "Epoch 98/100, Loss: 0.000402\n",
      "Epoch 99/100, Loss: 0.000402\n",
      "Epoch 100/100, Loss: 0.000402\n",
      "Extracting training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 430/430 [00:08<00:00, 48.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting test features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 44.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training KNN classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\training_structures\\dino_train.py:197: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=True) # For mixed precision training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy (k=5): 30.6200%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 430/430 [00:09<00:00, 45.21it/s, loss=0.676]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.9978, Val Loss: 0.7293, Val Acc: 75.54%\n",
      "Saved best model with validation accuracy: 75.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 430/430 [00:10<00:00, 41.99it/s, loss=0.881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.6748, Val Loss: 0.6310, Val Acc: 78.98%\n",
      "Saved best model with validation accuracy: 78.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 430/430 [00:10<00:00, 42.15it/s, loss=0.603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.5804, Val Loss: 0.5675, Val Acc: 80.52%\n",
      "Saved best model with validation accuracy: 80.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 430/430 [00:10<00:00, 41.89it/s, loss=0.396]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.5114, Val Loss: 0.5304, Val Acc: 82.20%\n",
      "Saved best model with validation accuracy: 82.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 430/430 [00:10<00:00, 41.01it/s, loss=0.54] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.4581, Val Loss: 0.4972, Val Acc: 83.42%\n",
      "Saved best model with validation accuracy: 83.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 430/430 [00:10<00:00, 40.97it/s, loss=0.41] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.4173, Val Loss: 0.4797, Val Acc: 83.72%\n",
      "Saved best model with validation accuracy: 83.72%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 430/430 [00:10<00:00, 41.02it/s, loss=0.579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.3865, Val Loss: 0.4636, Val Acc: 84.10%\n",
      "Saved best model with validation accuracy: 84.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 430/430 [00:10<00:00, 41.22it/s, loss=0.446]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.3636, Val Loss: 0.4605, Val Acc: 84.58%\n",
      "Saved best model with validation accuracy: 84.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 430/430 [00:10<00:00, 41.38it/s, loss=0.299]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.3480, Val Loss: 0.4560, Val Acc: 84.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 430/430 [00:10<00:00, 41.28it/s, loss=0.363]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.3393, Val Loss: 0.4542, Val Acc: 84.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\training_structures\\dino_train.py:285: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 80.38%\n",
      "Collecting samples from dataloader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 66.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected digits for visualization: [5, 8]\n",
      "Collecting samples from dataloader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 58.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing digits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Collecting samples from dataloader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:01<00:00, 67.87it/s]\n",
      "c:\\Users\\Ward\\anaconda3\\envs\\multibench\\Lib\\site-packages\\sklearn\\manifold\\_t_sne.py:1162: FutureWarning: 'n_iter' was renamed to 'max_iter' in version 1.5 and will be removed in 1.7.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing digits: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Applying t-SNE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ward\\Desktop\\vub-github\\Thesis-project\\AVMNIST_Experiments\\other_ssl\\autoencoder\\../..\\utils\\visualisations.py:750: UserWarning: Tight layout not applied. The left and right margins cannot be made large enough to accommodate all Axes decorations.\n",
      "  plt.tight_layout()\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "masked_autoencoder_conv = MaskedConvAutoencoder(output_dim=256, mask_ratio=0.75)\n",
    "training_loop(masked_autoencoder_conv, seeds=[1, 2, 3], model_name=\"masked_autoencoder_conv\", epochs=100, train_fn=train_masked_autoencoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multibench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
